{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Trabalho Final de ALN #\n",
    "\n",
    "## Nome : Felipe Eduardo dos Santos - 2017021223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolução de sistemas lineares por métodos iterativos\n",
    "\n",
    "Vamos ver o método do gradiente para aproximar soluções de sistemas do tipo : \n",
    "\n",
    "$$Ax=b$$\n",
    "$$\\ A\\in\\mathbb{R}^{n\\times n}$$\n",
    "$$x,b\\in\\mathbb{R}^n$$\n",
    "\n",
    "Onde A é simétrica definida positiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Métodos de Descida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Minimização e solução dos sitema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Começamos analisando a forma quadrática:\n",
    "\n",
    "$$\n",
    "J(x) = \\frac{1}{2}x^tAx-x^tb\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2}\\sum_{i,j=1}^n a_{ij}x_ix_j - \\sum_{i=1}^nb_ix_i\n",
    "$$\n",
    "\n",
    "Vamos começar avaliando o gradiente da forma, note que:\n",
    "\n",
    "$$\n",
    "\\nabla J(x) = (\\frac{\\partial}{\\partial x_1}J(x) ,...\\ , \\frac{\\partial}{\\partial x_n}J(x))\n",
    "$$\n",
    "\n",
    "<center> E portanto,</center>\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_k}J(x)\n",
    "= a_{k1}x_1 + ... + a_{kn}x_n - b_k \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla J(x) = Ax-b\n",
    "$$\n",
    "\n",
    "Se A é definida positiva, $\\nabla J(x)=0$ é ponto de mínimo como demonstrado no livro, logo, minimizar $J(x)$ é encontrar $x$ satisfazendo $Ax=b$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Espírito dos algoritmos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritmos de descidas baseados em minimizar $J$ consitem em, dar um chute inicial $x^{(0)}$ e gerar uma sequência $\\{x^{(n)}\\}_{n\\in\\mathbb{N}}$ de forma que $J(x^{(k+1)})<J(x^{(k)})$ assim eventualmente chegando a um $x^{(k)}$ sulficientemente próximo da solução do sistema.\n",
    "\n",
    "No caso dado um chute inicial $x^{(0)}$ metodos de descida consistem em encontrar uma direção $p^{(k)}$ e um coeficiente $\\alpha_k$ que faça nosso chute ficar cada vez mais próximo fazendo:\n",
    "\n",
    "$$\n",
    "x^{(k+1)} = x^{(k)} + \\alpha_k p^{(k)}\n",
    "$$\n",
    "\n",
    "Pelo teorema 7.4.5 do Livro sabemos que para o $J$ definido temos um alfa exato.\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\frac{(p^{(k)})^t r^{(k)}}{(p^{(k)})^t A p^{(k)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "r^{(k)} = b-Ax^{(k)}\n",
    "$$\n",
    "\n",
    "A escolha da direção a ser seguida caracteriza diversos algoritmos diferentes.\n",
    "\n",
    "Vamos fazer então uma função que representa um metodo de descida genérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descentAlg(x0, A, b, method, tol=1e-10, maxIter=int(1e5), printIters=True, printSteps=False):\n",
    "\n",
    "    # Limitamos o numero de iterações\n",
    "    for _ in range(maxIter):\n",
    "        if printSteps:\n",
    "            print(x0)\n",
    "        # residuo\n",
    "        r = b - A @ x0\n",
    "\n",
    "        # Caso o erro for pequeno o suficiente, paramos\n",
    "        if np.linalg.norm(r) <= tol:\n",
    "            if printIters:\n",
    "                print(f\"Converged in {_} iterations\")\n",
    "            break\n",
    "        \n",
    "        # Escolhemos a direção de descida e\n",
    "        # calculamos o alpha para andar na linha escolhida\n",
    "        pk, alpha = method(r, A)\n",
    "\n",
    "        # Atualizamos nossa aproximação\n",
    "        x0 = x0 + alpha * pk \n",
    "\n",
    "    return x0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Steepest Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A ideia é usar a direção de busca sendo a direção do resíduo, isto é, $p^{(k)}=r^{(k)}$, ou seja, como $r^{(k)} = -\\nabla J(x^{(k)})$ estamos descendo na direção mais íngreme, que é dada pelo oposto do gradiente.\n",
    "\n",
    " Para isso definiremos nosas funções que escolhem a direção e o alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradD(r,A):\n",
    "    # A direção é a do residuo que é o gradiente\n",
    "    pk = r\n",
    "\n",
    "    # Alpha portanto se torna\n",
    "    alpha = (r @ r) / (r @ (A @ r))\n",
    "\n",
    "    return pk, alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Exemplos simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 34 iterations\n",
      "[0.33333333 0.33333333]\n",
      "Converged in 1 iterations\n",
      "[0.33333333 0.33333333]\n",
      "Converged in 37 iterations\n",
      "[0.33333333 0.33333333]\n",
      "Converged in 26 iterations\n",
      "[0.33333333 0.33333333]\n",
      "Converged in 1 iterations\n",
      "[0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2,1],\n",
    "            [1,2]])\n",
    "            \n",
    "b = np.array([1,1])\n",
    "\n",
    "x0_1 = np.array([-543,99])\n",
    "x0_2 = np.array([0,0])\n",
    "x0_3 = np.array([-5,3])\n",
    "x0_4 = np.array([1,-7])\n",
    "\n",
    "\n",
    "print(descentAlg(x0_1, A, b, gradD))\n",
    "print(descentAlg(x0_2, A, b, gradD))\n",
    "print(descentAlg(x0_3, A, b, gradD))\n",
    "print(descentAlg(x0_4, A, b, gradD))\n",
    "print(descentAlg(b, A, b, gradD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Gradiente conjugado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f544ce1a915a9875fad91c894e2c0bcad4b7a79945aa6027ef3ad27810072aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
